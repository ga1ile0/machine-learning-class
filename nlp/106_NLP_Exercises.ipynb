{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercises\n",
    "\n",
    "We have five exercises in this section. The exercises are:\n",
    "1. Build your own tokenizer, where you need to implement two functions to implement a tokenizer based on regular expression.\n",
    "2. Get tags from Trump speech.\n",
    "3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "4. Build your own Bag Of Words implementation using tokenizer created before.\n",
    "5. Build a 5-gram model and clean up the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Build your own tokenizer\n",
    "\n",
    "Build two different tokenizers:\n",
    "- ``tokenize_sentence``: function tokenizing text into sentences,\n",
    "- ``tokenize_word``: function tokenizing text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences:\n",
      "['Here we go again.', 'I was supposed to add this text later.', \"Well, it's 10.p.m. here, and I'm actually having fun making this course. :o\", 'I hope you are getting along fine with this presentation, I really did try.', 'And one last sentence, just so you can test you tokenizers better.']\n",
      "Tokenized words:\n",
      "['Here', 'we', 'go', 'again', '.', 'I', 'was', 'supposed', 'to', 'add', 'this', 'text', 'later', '.', 'Well', ',', \"it's\", '10.p.m.', 'here', ',', 'and', 'I', \"'\", 'm', 'actually', 'having', 'fun', 'making', 'this', 'course', '.', ':o', 'I', 'hope', 'you', 'are', 'getting', 'along', 'fine', 'with', 'this', 'presentation', ',', 'I', 'really', 'did', 'try', '.', 'And', 'one', 'last', 'sentence', ',', 'just', 'so', 'you', 'can', 'test', 'you', 'tokenizers', 'better', '.']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def tokenize_words(text: str) -> list:\n",
    "   \n",
    "    text = re.sub(r'\\\\\\s*\\n', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'(:o)(?=\\w)', r'\\1 ', text)\n",
    "    \n",
    "    pattern = r'\\d+\\.[ap]\\.m\\.|\\:\\w|\\b[a-zA-Z]\\b|\\b\\w+(?:[-\\']?\\w+)*\\b|[^\\w\\s]'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "def tokenize_sentence(text: str) -> list:\n",
    "\n",
    "    text = re.sub(r'\\\\\\s*\\n\\s*', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'(\\d+)\\.([ap])\\.m\\.', r'\\1<TIMEABBR>\\2<TIMEABBR>m<TIMEABBR>', text)\n",
    "    \n",
    "    text = re.sub(r'(\\. :o) ([A-Z])', r'\\1<SENTBREAK>\\2', text)\n",
    "    \n",
    "    pattern = r'[^.!?]*[.!?](?:\\s+:o)?(?:<SENTBREAK>)?'\n",
    "    sentences = re.findall(pattern, text)\n",
    "    \n",
    "    result = []\n",
    "    for s in sentences:\n",
    "        if s.strip():\n",
    "            s = s.strip()\n",
    "            s = s.replace('<TIMEABBR>', '.')\n",
    "            s = s.replace('<SENTBREAK>', '')\n",
    "            result.append(s)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "text = \"Here we go again. I was supposed to add this text later.\\\n",
    "Well, it's 10.p.m. here, and I'm actually having fun making this course. :o\\\n",
    "I hope you are getting along fine with this presentation, I really did try.\\\n",
    "And one last sentence, just so you can test you tokenizers better.\"\n",
    "\n",
    "print(\"Tokenized sentences:\")\n",
    "print(tokenize_sentence(text))\n",
    "\n",
    "print(\"Tokenized words:\")\n",
    "print(tokenize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Get tags from Trump speech using NLTK\n",
    "\n",
    "You should use the ``trump.txt`` file, read it and find the tags for each word. Use NLTK for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ga1ile0/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ga1ile0/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ga1ile0/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/ga1ile0/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 tagged words:\n",
      "Thank: NNP\n",
      "you: PRP\n",
      "very: RB\n",
      "much: RB\n",
      ".: .\n",
      "Mr.: NNP\n",
      "Speaker: NNP\n",
      ",: ,\n",
      "Mr.: NNP\n",
      "Vice: NNP\n",
      "President: NNP\n",
      ",: ,\n",
      "Members: NNP\n",
      "of: IN\n",
      "Congress: NNP\n",
      ",: ,\n",
      "the: DT\n",
      "First: NNP\n",
      "Lady: NNP\n",
      "of: IN\n",
      "\n",
      "Tag frequency distribution:\n",
      "NN: 667\n",
      "IN: 549\n",
      "DT: 456\n",
      "JJ: 381\n",
      "NNS: 358\n",
      "NNP: 322\n",
      ",: 312\n",
      ".: 287\n",
      "PRP: 284\n",
      "VB: 283\n",
      "CC: 241\n",
      "RB: 231\n",
      "PRP$: 175\n",
      "VBP: 172\n",
      "TO: 150\n",
      "VBN: 127\n",
      "MD: 114\n",
      "VBZ: 99\n",
      "VBG: 95\n",
      "VBD: 86\n",
      "CD: 60\n",
      "NNPS: 36\n",
      "WDT: 31\n",
      "WP: 26\n",
      "JJR: 26\n",
      "WRB: 21\n",
      "POS: 19\n",
      "RP: 19\n",
      ":: 17\n",
      "JJS: 9\n",
      "RBR: 8\n",
      "PDT: 5\n",
      "$: 5\n",
      "'': 5\n",
      "``: 3\n",
      "EX: 3\n",
      "UH: 2\n",
      "WP$: 1\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "file = open(\"../datasets/trump.txt\", \"r\", encoding=\"utf-8\") \n",
    "trump = file.read()\n",
    "words = word_tokenize(trump)\n",
    "\n",
    "tagged_words = pos_tag(words)\n",
    "\n",
    "print(\"First 20 tagged words:\")\n",
    "for word, tag in tagged_words[:20]:\n",
    "    print(f\"{word}: {tag}\")\n",
    "\n",
    "tag_counts = {}\n",
    "for _, tag in tagged_words:\n",
    "    if tag in tag_counts:\n",
    "        tag_counts[tag] += 1\n",
    "    else:\n",
    "        tag_counts[tag] = 1\n",
    "\n",
    "print(\"\\nTag frequency distribution:\")\n",
    "for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "\n",
    "Please use Python list features to get the last 10 sentences and display nouns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 sentences from Trump's speech:\n",
      "\n",
      "Sentence 1: When we fulfill this vision, when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American greatness began.\n",
      "Nouns: vision, years, freedom, tonight, chapter, greatness\n",
      "\n",
      "Sentence 2: The time for small thinking is over.\n",
      "Nouns: time, thinking\n",
      "\n",
      "Sentence 3: The time for trivial fights is behind us.\n",
      "Nouns: time, fights\n",
      "\n",
      "Sentence 4: We just need the courage to share the dreams that fill our hearts, the bravery to express the hopes that stir our souls, and the confidence to turn those hopes and those dreams into action.\n",
      "Nouns: courage, dreams, hearts, bravery, hopes, souls, confidence, hopes, dreams, action\n",
      "\n",
      "Sentence 5: From now on, America will be empowered by our aspirations, not burdened by our fears; inspired by the future, not bound by failures of the past; and guided by our vision, not blinded by our doubts.\n",
      "Nouns: aspirations, fears, future, failures, past, vision, doubts\n",
      "\n",
      "Sentence 6: I am asking all citizens to embrace this renewal of the American spirit.\n",
      "Nouns: citizens, renewal, spirit\n",
      "\n",
      "Sentence 7: I am asking all Members of Congress to join me in dreaming big and bold, and daring things for our country.\n",
      "Nouns: Members, things, country\n",
      "\n",
      "Sentence 8: I am asking everyone watching tonight to seize this moment.\n",
      "Nouns: tonight, moment\n",
      "\n",
      "Sentence 9: Believe in yourselves, believe in your future, and believe, once more, in America.\n",
      "Nouns: yourselves, future\n",
      "\n",
      "Sentence 10: Thank you, God bless you, and God bless the United States.\n",
      "No nouns in this sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "file = open(\"../datasets/trump.txt\", \"r\", encoding='utf-8') \n",
    "trump = file.read() \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(trump)\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "last_ten_sentences = sentences[-10:]\n",
    "\n",
    "print(f\"Last {len(last_ten_sentences)} sentences from Trump's speech:\\n\")\n",
    "\n",
    "for i, sentence in enumerate(last_ten_sentences):\n",
    "    nouns = [token.text for token in sentence if token.pos_ == \"NOUN\"]\n",
    "    \n",
    "    print(f\"Sentence {i+1}: {sentence.text.strip()}\")\n",
    "    if nouns:\n",
    "        print(f\"Nouns: {', '.join(nouns)}\")\n",
    "    else:\n",
    "        print(\"No nouns in this sentence.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Build your own Bag Of Words implementation using tokenizer created before \n",
    "\n",
    "You need to implement following methods:\n",
    "\n",
    "- ``fit_transform`` - gets a list of strings and returns matrix with it's BoW representation\n",
    "- ``get_features_names`` - returns list of words corresponding to columns in BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1]\n",
      " [1 0 0 0 2 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 2 1 0 0 1 0 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Basic BoW implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__vocabulary = []  \n",
    "        self.__feature_matrix = None  \n",
    "    \n",
    "    def fit_transform(self, corpus: list):\n",
    "        \n",
    "        tokenized_docs = [tokenize_words(doc) for doc in corpus]\n",
    "        \n",
    "        all_words = []\n",
    "        for doc in tokenized_docs:\n",
    "            all_words.extend([word.lower() for word in doc if word.isalnum()])\n",
    "        \n",
    "        self.__vocabulary = sorted(list(set(all_words)))\n",
    "        \n",
    "        matrix = np.zeros((len(corpus), len(self.__vocabulary)), dtype=int)\n",
    "        \n",
    "        for doc_idx, doc in enumerate(tokenized_docs):\n",
    "            doc_words = [word.lower() for word in doc if word.isalnum()]\n",
    "            for word in doc_words:\n",
    "                if word in self.__vocabulary:\n",
    "                    word_idx = self.__vocabulary.index(word)\n",
    "                    matrix[doc_idx, word_idx] += 1\n",
    "        \n",
    "        self.__feature_matrix = matrix\n",
    "        return matrix\n",
    "    \n",
    "    def get_feature_names(self) -> list:\n",
    "         \n",
    "        return self.__vocabulary\n",
    "\n",
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]    \n",
    "    \n",
    "vectorizer = BagOfWords()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "\n",
    "vectorizer.get_feature_names()\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Build a 5-gram model and clean up the results.\n",
    "\n",
    "There are three tasks to do:\n",
    "1. Use 5-gram model instead of 3.\n",
    "2. Change to capital letter each first letter of a sentence.\n",
    "3. Remove the whitespace between the last word in a sentence and . ! or ?.\n",
    "\n",
    "Hint: for 2. and 3. implement a function called ``clean_generated()`` that takes the generated text and fix both issues at once. It could be easier to fix the text after it's generated rather then doing some changes in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "wall_street = text7.tokens\n",
    "\n",
    "import re\n",
    "\n",
    "tokens = wall_street\n",
    "\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "tokens = cleanup()\n",
    "\n",
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams\n",
    "\n",
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts\n",
    "\n",
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sequences: ['a bill because of', 'amphobiles according to Brooke', 'as the 1980s bull', 'night if the magazine', '31 1993 at a']\n",
      "A bill because of what it views as an undesirable intrusion into the affairs of industry but the 300-113 vote suggests that supporters have the potential 0 to override a veto.The broader question is where the Senate stands on the issue.While the Senate Commerce Committee has approved legislation similar to the House bill on airline leveraged buy-outs the measure has n't yet come to the full floor.Although the legislation would apply to acquisitions involving any major airline it is aimed at giving the Transportation Department the chance to review in advance transactions financed by large amounts of debt.The rating concern said 0 the coupon rate has n't yet been fixed but will probably be set at around 8.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def clean_generated(text: str) -> str:\n",
    "   \n",
    "    text = re.sub(r'\\s+([.!?])', r'\\1', text)\n",
    "    \n",
    "    sentences = re.split(r'([.!?])\\s+', text)\n",
    "    \n",
    "    cleaned_text = \"\"\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        if i == 0:\n",
    "            cleaned_text += sentences[i][0].upper() + sentences[i][1:]\n",
    "        elif sentences[i] in ['.', '!', '?']:\n",
    "            cleaned_text += sentences[i]\n",
    "        else:\n",
    "            cleaned_text += sentences[i][0].upper() + sentences[i][1:]\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "N=5 \n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "some_keys = random.sample(list(counts.keys()), 5)\n",
    "print(\"Available sequences:\", some_keys)\n",
    "\n",
    "start_seq = some_keys[0]\n",
    "\n",
    "generated = start_seq \n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "generated = clean_generated(generated)\n",
    "\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
